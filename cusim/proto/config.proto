// Copyright (c) 2021 Jisang Yoon
// All rights reserved.
//
// This source code is licensed under the Apache 2.0 license found in the
// LICENSE file in the root directory of this source tree.

syntax = "proto2";


// option for data preprocessing
message IoUtilsConfigProto {
  // logging levels in python and C++
  optional int32 py_log_level = 1 [default = 2];
  optional int32 c_log_level = 2 [default = 2];

  // number of chunk lines to preprocess (txt => hdf5 format) data
  optional int32 chunk_lines = 3 [default = 100000];

  // number of concurrent threads in data preprocessing
  optional int32 num_threads = 4 [default = 4];

  // convert charater to lower case if true
  optional bool lower = 5 [default = true];
}


// option for LDA model
message CuLDAConfigProto {
  // logging levels in python and C++
  optional int32 py_log_level = 1 [default = 2];
  optional int32 c_log_level = 2 [default = 2];

  // raw data path (format from https://archive.ics.uci.edu/ml/datasets/bag+of+words)
  optional string data_path = 7;

  // preprocessed data path (hdf5 format)
  // if empty, make temporary directory
  optional string processed_data_path = 6;

  // vocabulary path
  required string keys_path = 16;

  // skip preprocess (there should be already preprocessed hdf5 format) if true
  optional bool skip_preprocess = 8;

  // path to store gamma in E step
  // if empty, make temporary directory
  optional string gamma_path = 17;

  // reuse gamma from previous epoch if true
  // if false, initiate gamma as Figure 6 in https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf
  optional bool reuse_gamma = 18;

  // number of topics
  optional int32 num_topics = 3 [default = 10];

  // block dimension in CUDA
  // should be multiple of WARP_SIZE (=32)
  optional int32 block_dim = 4 [default = 32];

  // set the number blocks as num_blocks * block_dim = physical_cores_in_GPU * hyper_threads
  optional int32 hyper_threads = 5 [default = 100];

  // batch size in training
  optional int32 batch_size = 10 [default = 1000000];

  // number of epochs in training
  optional int32 epochs = 11 [default = 10];

  // number of iterations in each E step
  optional int32 num_iters_in_e_step = 12 [default = 5];

  // validation ratio, should be between 0 and 1
  optional double vali_p = 13 [default = 0.2];

  // random seed
  optional int32 seed = 14 [default = 777];

  // remove all tempory directorys generated by package when program finnished if true
  optional bool remove_tmp = 19 [default = true];

  optional IoUtilsConfigProto io = 15;
}

// options for loading pretrained w2v model
// can load w2v model file generated by gensim or original w2v code by Google
message W2VPretrainedModel {
  optional string filename = 1;
  optional bool no_header = 2;
  optional bool binary = 3;
  optional bool symmetry = 4;
}


// option for training Word2Vec model
message CuW2VConfigProto {
  // logging levels in python and C++
  optional int32 py_log_level = 1 [default = 2];
  optional int32 c_log_level = 2 [default = 2];

  // raw data path (stream txt format)
  optional string data_path = 7;

  // path to save preprocessed data (hdf5 format)
  optional string processed_data_dir = 6;

  // skip data preprocessing (therefore, there should be
  // already preprocessed hdf5 format file) if true
  optional bool skip_preprocess = 8;

  // number of embedding dimensions
  optional int32 num_dims = 3 [default = 50];

  // block_dim in CUDA
  optional int32 block_dim = 4 [default = 32];

  // set number of blocks as num_blocks * block_dim = physical_cores_in_GPU * hyper_threads
  optional int32 hyper_threads = 5 [default = 100];

  // generate vocabulary with words appreared in corpus at least word_min_count times
  optional int32 word_min_count = 9 [default = 5];

  // batch size and number of epochs in training
  optional int32 batch_size = 10 [default = 1000000];
  optional int32 epochs = 11 [default = 10];

  // seed fields
  optional int32 seed = 14 [default = 777];

  // random table size in negative sampling
  optional int32 random_size = 12 [default = 100000000];

  // number of negative samples
  // if zero, it uses hierarchical softmax
  optional int32 neg = 17 [default = 10];

  // weight in negative sampling will be word_count ** count_power for each word
  // default value 0.75 is recommended in w2v paper
  optional double count_power = 18 [default = 0.75];

  // if true, train skip gram model, else train cbow model
  optional bool skip_gram = 19 [default = true];

  // if true, use average context vector in cbow model
  // else use summation of context vectors
  optional bool cbow_mean = 20 [default = true];

  // learning rate
  optional double lr = 21 [default = 0.001];

  // window size in both skip gram and cbow model
  optional int32 window_size = 22 [default = 5];

  // remove all tempory directorys generated by package when program finnished if true
  optional bool remove_tmp = 26 [default = true];

  optional IoUtilsConfigProto io = 24;
  optional W2VPretrainedModel pretrained_model = 25;
}
